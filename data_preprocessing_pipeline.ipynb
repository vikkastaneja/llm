{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c946d84",
   "metadata": {},
   "source": [
    "## Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a793a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in the file: 20479\n"
     ]
    }
   ],
   "source": [
    "# Use the tiktokenizer to create tokens\n",
    "with open(\"the-verdict.txt\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Number of characters in the file: {len(raw_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc69698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "# We will use tiktoken, which was used by GPT2\n",
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfb88948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialzie tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0191e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# Encode the text from \"the-verdict.txt\" using tokenizer\n",
    "integers = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(len(integers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb54a260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Generate back the string and compare with original raw text\n",
    "strings = tokenizer.decode(integers)\n",
    "print(len(strings))\n",
    "print(len(strings) == len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a41155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "['Ak', 'w', 'ir', 'w', ' ', 'ier']\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "# Let's see how it decodes the unknown words\n",
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "print([tokenizer.decode([i]) for i in integers])\n",
    "print(tokenizer.decode(integers))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fdf47",
   "metadata": {},
   "source": [
    "### Step 2: CREATE INPUT-TARGET PAIRS USING DATALOADERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9dda7",
   "metadata": {},
   "source": [
    "##### STEPS:\n",
    "<li>Tokenize entire text</li>\n",
    "<li>Use a sliding window to chunk the book into overlapping sequence of max_length</li>\n",
    "<li>Return the total number of rows in the dataset</li>\n",
    "<li>Return a single row from the dataset</li>\n",
    "\n",
    "<b> For efficient data loader implementation, we will use PyTorch's built-in dataset and dataloader classes.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e169a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "# First we will create a Dataset class that will be used by DataLoader to extract the data from 'raw_text' efficiently\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entier dataset\n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use sliding window to chunk the book into overlapping sequence of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length - 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4e769d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above Dataset class will be served as input to create datasets using dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "import tiktoken\n",
    "def dataloader_v1(text, batch_size=4, max_length = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDataset(text=text, tokenizer=tokenizer, stride=stride, max_length=max_length)\n",
    "\n",
    "    # Create Dataloader\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b97e8f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885]])]\n"
     ]
    }
   ],
   "source": [
    "# Convert dataloader into a python iterator to fetch next entry\n",
    "import torch\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "dataloader = dataloader_v1(text=raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "print(next(data_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c1494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464]])]\n",
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Let's pull two other data and confirm that stride is one.\n",
    "# The second element of first tensor should be first element of next tensor.\n",
    "# In reality, low stride leads to overfitting since the overlapped data is fed multiple times\n",
    "# It is good to have a stride = length of input vector\n",
    "for i in range(2):\n",
    "    print(next(data_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a681afd",
   "metadata": {},
   "source": [
    "### Step 3: TOKEN EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e851879e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create an embedding layer of vocab size and 256 length vector for each token\n",
    "embedding_layer = torch.nn.Embedding(tokenizer.n_vocab, 256)\n",
    "print(embedding_layer.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0af80e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 4, 256])\n",
      "tensor([[[ 0.7709,  0.0156, -0.6348,  ..., -1.4782,  0.9493,  1.8082],\n",
      "         [-0.6345,  0.0218,  0.9705,  ..., -0.5795,  3.3748, -0.6350],\n",
      "         [ 0.1139, -1.8634, -0.5039,  ..., -0.7990,  1.0489,  0.1555],\n",
      "         [ 0.2059, -0.9976, -0.1893,  ...,  0.3613, -0.0398,  1.0546]],\n",
      "\n",
      "        [[ 1.6244,  2.6661, -0.0340,  ...,  0.2866, -0.3518,  0.3947],\n",
      "         [-0.9423,  0.6046,  0.0386,  ...,  0.2155, -0.0626,  0.8304],\n",
      "         [-1.1962,  0.8808, -1.1823,  ...,  0.2864,  0.5574,  1.4500],\n",
      "         [-0.5577,  2.1031, -0.3253,  ..., -0.4192,  0.8419, -0.4898]],\n",
      "\n",
      "        [[-0.1929, -1.5585,  0.6899,  ..., -1.7189, -1.4585,  1.4830],\n",
      "         [ 0.1032, -1.4636,  0.1693,  ..., -0.0269, -0.7163, -1.6858],\n",
      "         [ 0.0572, -0.8663,  1.1309,  ..., -0.5880, -0.8733, -0.0704],\n",
      "         [-1.0115, -1.3127, -2.2282,  ..., -0.9161, -0.2683, -0.1681]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0916, -0.2895,  1.4656,  ...,  0.8897, -0.7974,  0.0946],\n",
      "         [ 0.1434,  0.2653,  1.3709,  ...,  1.5398,  1.6182, -0.9930],\n",
      "         [-0.5308,  0.1815,  2.0209,  ...,  1.9872, -0.3251, -0.2384],\n",
      "         [ 0.4651, -1.3444, -0.9234,  ...,  0.3398, -1.1386,  0.6367]],\n",
      "\n",
      "        [[ 1.0880,  0.3975, -0.1070,  ...,  0.1630,  0.6502, -1.6698],\n",
      "         [ 0.5331,  1.2522,  0.5027,  ..., -0.4795, -0.5693, -0.8512],\n",
      "         [-1.5591,  0.5825,  0.1539,  ...,  1.9032,  1.5179, -0.2417],\n",
      "         [ 1.0712,  1.7248, -0.8348,  ..., -1.9234, -1.0234,  0.9347]],\n",
      "\n",
      "        [[-1.5591,  0.5825,  0.1539,  ...,  1.9032,  1.5179, -0.2417],\n",
      "         [-0.7810,  0.6791,  0.4156,  ..., -0.1135,  0.7750, -0.1217],\n",
      "         [ 0.4479, -1.9082,  1.4882,  ..., -1.5341,  0.2086, -0.1276],\n",
      "         [-0.0391, -0.8193,  0.5661,  ...,  1.1724, -1.1427,  0.1804]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adam\\AppData\\Local\\Temp\\ipykernel_33056\\1738613274.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding_vector = embedding_layer(torch.tensor(integers))\n"
     ]
    }
   ],
   "source": [
    "# Now embedding layer has 50257x256\n",
    "# Retrieve token embedding vector for raw text tokens\n",
    "# We need to get input vectors as 8x4 --> 8 inputs with 4 token/input\n",
    "max_length = 4\n",
    "dataloader = dataloader_v1(text = raw_text, batch_size=8, max_length=max_length, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "integers, _ = next(data_iter)\n",
    "print(integers)\n",
    "print(integers.shape)\n",
    "embedding_vector = embedding_layer(torch.tensor(integers))\n",
    "print(embedding_vector.shape)\n",
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d145dba",
   "metadata": {},
   "source": [
    "### Step 4: POSITIONAL EMBEDDING (ENCODING WORD POSITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41e24fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1521,  0.6823, -0.0041,  ..., -0.7542, -1.6720,  1.7446],\n",
      "        [ 0.0170,  0.1187,  0.5695,  ..., -0.0959,  0.1300,  1.3664],\n",
      "        [ 0.1792,  0.9517,  0.0896,  ...,  1.2800, -1.1555, -0.4063],\n",
      "        [-0.8819, -0.4398, -0.5946,  ...,  0.9804, -0.9617,  0.8497]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# In positional embedding, we will have another layer called token embedding layer like in token embedding vector\n",
    "pos_embedding_layer = torch.nn.Embedding(max_length, 256) # Because we want just one input length, creates a lookup table \n",
    "\n",
    "print(pos_embedding_layer.weight.shape)\n",
    "print(pos_embedding_layer.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13dfe52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "torch.Size([4, 256])\n",
      "tensor([[ 0.1521,  0.6823, -0.0041,  ..., -0.7542, -1.6720,  1.7446],\n",
      "        [ 0.0170,  0.1187,  0.5695,  ..., -0.0959,  0.1300,  1.3664],\n",
      "        [ 0.1792,  0.9517,  0.0896,  ...,  1.2800, -1.1555, -0.4063],\n",
      "        [-0.8819, -0.4398, -0.5946,  ...,  0.9804, -0.9617,  0.8497]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Just reshape\n",
    "idx = torch.arange(max_length)\n",
    "print(idx)\n",
    "pos_embedding = pos_embedding_layer(idx)\n",
    "print(pos_embedding.shape)\n",
    "print(pos_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c76a0b4",
   "metadata": {},
   "source": [
    "### Step 5: INPUT LAYER (TO FEED TO LLM FOR TRAINING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89276425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "tensor([[[ 0.9230,  0.6979, -0.6388,  ..., -2.2324, -0.7226,  3.5528],\n",
      "         [-0.6174,  0.1405,  1.5400,  ..., -0.6754,  3.5048,  0.7314],\n",
      "         [ 0.2932, -0.9117, -0.4143,  ...,  0.4810, -0.1067, -0.2508],\n",
      "         [-0.6760, -1.4374, -0.7839,  ...,  1.3416, -1.0015,  1.9043]],\n",
      "\n",
      "        [[ 1.7765,  3.3485, -0.0380,  ..., -0.4676, -2.0238,  2.1393],\n",
      "         [-0.9253,  0.7233,  0.6081,  ...,  0.1196,  0.0674,  2.1969],\n",
      "         [-1.0169,  1.8325, -1.0928,  ...,  1.5663, -0.5981,  1.0437],\n",
      "         [-1.4396,  1.6633, -0.9199,  ...,  0.5611, -0.1198,  0.3599]],\n",
      "\n",
      "        [[-0.0408, -0.8761,  0.6859,  ..., -2.4731, -3.1304,  3.2276],\n",
      "         [ 0.1202, -1.3449,  0.7388,  ..., -0.1229, -0.5863, -0.3193],\n",
      "         [ 0.2365,  0.0854,  1.2205,  ...,  0.6920, -2.0288, -0.4767],\n",
      "         [-1.8934, -1.7525, -2.8228,  ...,  0.0643, -1.2300,  0.6816]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0605,  0.3928,  1.4615,  ...,  0.1355, -2.4694,  1.8392],\n",
      "         [ 0.1604,  0.3839,  1.9404,  ...,  1.4439,  1.7482,  0.3734],\n",
      "         [-0.3515,  1.1333,  2.1105,  ...,  3.2672, -1.4806, -0.6447],\n",
      "         [-0.4168, -1.7842, -1.5179,  ...,  1.3201, -2.1003,  1.4864]],\n",
      "\n",
      "        [[ 1.2401,  1.0799, -0.1110,  ..., -0.5912, -1.0217,  0.0748],\n",
      "         [ 0.5501,  1.3709,  1.0722,  ..., -0.5754, -0.4393,  0.5153],\n",
      "         [-1.3798,  1.5342,  0.2435,  ...,  3.1832,  0.3623, -0.6481],\n",
      "         [ 0.1893,  1.2850, -1.4294,  ..., -0.9431, -1.9852,  1.7844]],\n",
      "\n",
      "        [[-1.4070,  1.2649,  0.1498,  ...,  1.1490, -0.1541,  1.5029],\n",
      "         [-0.7640,  0.7977,  0.9851,  ..., -0.2094,  0.9050,  1.2448],\n",
      "         [ 0.6272, -0.9565,  1.5778,  ..., -0.2541, -0.9469, -0.5339],\n",
      "         [-0.9210, -1.2591, -0.0284,  ...,  2.1527, -2.1045,  1.0301]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_layer = embedding_vector + pos_embedding\n",
    "print(input_layer.shape)\n",
    "print(input_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314914af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gensim-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
