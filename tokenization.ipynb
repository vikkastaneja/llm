{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5b5b39",
   "metadata": {},
   "source": [
    "## Read short story for tokenzation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e0f22",
   "metadata": {},
   "source": [
    "### Step 1: Create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9127b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Print total number of characters\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "\n",
    "# print first 100 characters\n",
    "print(raw_text[:99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddf32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world!', 'This', 'is', 'a', 'test,', 'working', 'now.']\n",
      "['Hello', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test,', ' ', 'working', ' ', 'now.']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expression library\n",
    "import re\n",
    "\n",
    "text = \"Hello world! This is a test, working now.\"\n",
    "\n",
    "# Split based on white spaces: use \\s as regex\n",
    "result = re.split(r'\\s', text) # This will remove the white space\n",
    "\n",
    "print(result)\n",
    "\n",
    "result = re.split(r'(\\s)', text) # This will include the white spaces\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd75c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', ',', '', ' ', 'working', ' ', 'now', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# Now we want to include punctuation characters (commas full stops and exclamation marks) also as separate strings\n",
    "result = re.split(r'(\\W)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae1db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'test', ',', 'working', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove empty strings and white space characters now\n",
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "517a0e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "# for the book text, there are ? -- : _ ; ' \" () are also present. Hence we need to consider them while splitting\n",
    "# Hence our tokenizer code is as follows, as of now. This is a simple tokenizer. \n",
    "# For LLMs, different tokenizer scheme is used which we will see later\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [t for t in preprocessed if t.strip()]\n",
    "print (len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec0194",
   "metadata": {},
   "source": [
    "### Step 2: Creating token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfe57be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# Get all unique words and sorted\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2447b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(!, 0)\n",
      "(\", 1)\n",
      "(', 2)\n",
      "((, 3)\n",
      "(), 4)\n",
      "(,, 5)\n",
      "(--, 6)\n",
      "(., 7)\n",
      "(:, 8)\n",
      "(;, 9)\n",
      "(?, 10)\n",
      "(A, 11)\n",
      "(Ah, 12)\n",
      "(Among, 13)\n",
      "(And, 14)\n",
      "(Are, 15)\n",
      "(Arrt, 16)\n",
      "(As, 17)\n",
      "(At, 18)\n",
      "(Be, 19)\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary\n",
    "vocab = {}\n",
    "counter = 0\n",
    "for word in all_words:\n",
    "    vocab[word] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "items = list(vocab.items())[:20]\n",
    "for key,value in items:\n",
    "    print(f\"({key}, {value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4882155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a simple tokenizer class that will be used to encode the given text to feed IDs to LLM and decode the Token IDs from LLM to convert back to text.\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        # For encoder\n",
    "        self.str_to_int = vocab\n",
    "\n",
    "        # For decoder, int to string mapping\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        # Get rid of spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6551156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(vocab['\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67f7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# initialize the tokenizer\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04aea15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b69b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# The encoder fails when the words are not in the vocabulary\u001b[39;00m\n\u001b[32m      2\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     11\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     12\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# The encoder fails when the words are not in the vocabulary\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbbe94",
   "metadata": {},
   "source": [
    "#### This shows that we need a large and diverse training set to extend the vocabulary when working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958857e",
   "metadata": {},
   "source": [
    "Another solution is to use special context tokens, user by GPT as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e036a73",
   "metadata": {},
   "source": [
    "### SPECIAL CONTEXT TOKENS <|unk|> and <|endoftext|> to existing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a683ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1414ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# print last 5 entries in the updated vocab\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6076c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated existing tokenizer class to include the new special tokens\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        # For encoder\n",
    "        self.str_to_int = vocab\n",
    "\n",
    "        # For decoder, int to string mapping\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Split\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "\n",
    "        # Remove white spaces\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "        # It item is not present in vocab, the token is 'unknown'\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        # Convert tokens to token IDs\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        # Get rid of spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0971e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20114cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', '', ' ', 'do', ' ', 'you', ' ', 'like', ' ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53e0f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c62376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afff1f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b0b09f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h4>GPT uses Byte Pair Encoding.</h4>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae298d",
   "metadata": {},
   "source": [
    "#### After understanding how BPE works in notes, it is fairly complicated algorithm to implement. Hence we will use an existing python library called 'tiktoken' that's fast [BPE tokenizer](https://github.com/openai/tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "849bc0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\adam\\common_python_environment\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\adam\\common_python_environment\\.venv\\lib\\site-packages (from tiktoken) (2025.9.18)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\adam\\common_python_environment\\.venv\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\adam\\common_python_environment\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\adam\\common_python_environment\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\adam\\common_python_environment\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\adam\\common_python_environment\\.venv\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3219e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20aa111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total token size: 50256\n",
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 850, 18250, 256, 3258, 2114, 1659, 617, 34680, 27271, 13]\n",
      "Decode of tokens for someunknownPlace [617, 34680, 27271]:  someunknownPlace\n"
     ]
    }
   ],
   "source": [
    "# Initialize BPE\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# someunknownPlace will be recognized as well which in our encoder fails\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sublit tarraces\"\n",
    "    \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print('total token size:', tokenizer.max_token_value)\n",
    "print(integers)\n",
    "\n",
    "# note that 50256 is the <|endoftext|> token and vocab list for gpt2 is 50k.\n",
    "# The actual size of words in English language is around 200k.\n",
    "# This shows BPE has considerably reduced the size of vocabulary.\n",
    "\n",
    "# The following line will demonstrate how 'someunknownPlace' has been tokenized to.\n",
    "print(\"Decode of tokens for someunknownPlace [617, 34680, 27271]:\", tokenizer.decode([617, 34680, 27271]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55380adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sublit tarracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "# Convert the token IDs back into the text\n",
    "print(tokenizer.decode(integers))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8feaf2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h5>Observations:</h5>\n",
    "<ol>\n",
    "  <li>'<|endoftext|>' token is assigned a relatively large token ID: 50256</li>\n",
    "  <li>BPE tokenizer encodes and decodes unknown words, e.g., 'someunknownPlace'</li>\n",
    "  <li>The algo underlying BPE breaks down out of vocab words into smaller subwords or even characters,<br/>which enables it to handle out of vocab words.</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea401fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n",
      "w ier\n",
      "Print each token:\n",
      "33901 Ak\n",
      "86 w\n",
      "343 ir\n",
      "86 w\n",
      "220  \n",
      "959 ier\n"
     ]
    }
   ],
   "source": [
    "# Simple example to illustrate how the BPE tokenizer delas with unknown tokens\n",
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "print(tokenizer.decode(integers))\n",
    "\n",
    "print(tokenizer.decode([86,220,959]))\n",
    "\n",
    "print('Print each token:')\n",
    "for t in integers:\n",
    "    txt = tokenizer.decode([t])\n",
    "    print(t, txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fa3ec",
   "metadata": {},
   "source": [
    "## Creating Input Target Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fb1b8",
   "metadata": {},
   "source": [
    "#### We will implement a data loader that fetches the input-target pairs using Sliding Window approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69498ea3",
   "metadata": {},
   "source": [
    "##### We will use the BPE tokenizer to tokenize The Verdict story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f5c911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5145\n",
      "40 I\n",
      "367  H\n",
      "2885 AD\n",
      "1464  always\n",
      "1807  thought\n",
      "3619  Jack\n",
      "402  G\n",
      "271 is\n",
      "10899 burn\n",
      "2138  rather\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = encoding.encode(raw_text)\n",
    "print(\"Vocabulary size:\", len(enc_text))\n",
    "\n",
    "# Print first 10 tokens\n",
    "for i in range(10):\n",
    "    print(enc_text[i], encoding.decode([enc_text[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79c3d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920, 2241, 287] [4920, 2241, 287, 257]\n",
      "Input: [290], Output/Target: 4920\n",
      "Input: [290, 4920], Output/Target: 2241\n",
      "Input: [290, 4920, 2241], Output/Target: 287\n",
      "Input: [290, 4920, 2241, 287], Output/Target: 257\n"
     ]
    }
   ],
   "source": [
    "# Now to demonstrate, remove first fifty tokens\n",
    "enc_sample = enc_text[50:]\n",
    "\n",
    "# Context size: How many tokens to have as an input to LLM, to predict next word. \n",
    "# The model is trained to look at a sequence of context_size tokens to predict next word in the sequence\n",
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "print(x, y)\n",
    "for i in range(context_size):\n",
    "    print(f\"Input: {x[:i+1]}, Output/Target: {y[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afa463c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290, 4920, 2241, 287] [4920, 2241, 287, 257]\n",
      "Input:  and, Output/Target:  established\n",
      "Input:  and established, Output/Target:  himself\n",
      "Input:  and established himself, Output/Target:  in\n",
      "Input:  and established himself in, Output/Target:  a\n"
     ]
    }
   ],
   "source": [
    "# Take the previous code and repeat for decoded tokens to text\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "print(x, y)\n",
    "for i in range(context_size):\n",
    "    print(f\"Input: {tokenizer.decode(x[:i+1])}, Output/Target: {tokenizer.decode([y[i]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cfc7cc",
   "metadata": {},
   "source": [
    "Now we have implemented the logic of sliding window to generate IO pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b34b9d",
   "metadata": {},
   "source": [
    "Now we need to utilize the same in more structured way and generate a data loader that we can generate in parallel on multiple CPUs that will generate pytorch tensors (multi dimensional arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923ec4f",
   "metadata": {},
   "source": [
    "### Implement a Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b2397",
   "metadata": {},
   "source": [
    "We will use Pytorch's Dataset and Dataloader classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37de97b",
   "metadata": {},
   "source": [
    "Step 1: Tokenize the entire text<br/>\n",
    "Step 2: Use the sliding window to chunk the book into overlapping sequences of max_length<br/>\n",
    "Step 3: Return the total number of rows in the dataset<br/>\n",
    "Step 4: Return a single row from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83728153",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The class is based on PyTorch Dataset class<br/>\n",
    "It defines how individual rows are fetched from the dataset<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47459de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Functions of this class is based on Pytorch Dataset class and is implemented based on the dataloader documentation in pytorch\n",
    "# See: https://docs.pytorch.org/docs/stable/data.html#map-style-datasets:~:text=style%20datasets.-,Map%2Dstyle%20datasets,-%23\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Step 1: Tokenize the entire dataset\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequence of max_length\n",
    "        # stride is jump so that next row picks beyond stride. Note that it is not context size\n",
    "        # max_length is also context size - number of elements in a row of tensor, it may be different than stride\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i + max_length]\n",
    "            output_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(output_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cee91",
   "metadata": {},
   "source": [
    "### Code for using above class (Create data loader)\n",
    "Step 1: Initialize the tokenizer<br/>\n",
    "Step 2: Create dataset<br/>\n",
    "Step 3: drop_last is set to true to drop the last batch if it is shorter than the batch_size to prevent loss spikes during training<br/>\n",
    "Step 4: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a421298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \"\"\"\n",
    "    Dataloader to do preprocessing in parallel\n",
    "\n",
    "    Args:\n",
    "        txt (string): Input text\n",
    "        batch_size (int): Number of threads - number of batches that model processes before updating its parameters\n",
    "        max_length (int): Context length\n",
    "        stride (int): How much to skip the tokens for each row - shift within batch, This means the next batch will have first row which is already shifted stride*batch_size places, see example below.\n",
    "        shuffle (boolean):\n",
    "        drop_last (boolean):\n",
    "        num_workers (int): Number of CPU threads - parallel processing\n",
    "    \"\"\"\n",
    "    # Initilize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader, which is going to look at the GPTDatasetV1.__getitem__ function and create the IO tensors\n",
    "    # dataloader will help us to load data in parallel and analyze multiple datasets at one time\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b96bb",
   "metadata": {},
   "source": [
    "Test the dataloader with batch size = 1 and context size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "347c81d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464],\n",
      "        [2885, 1464, 1807, 3619]]), tensor([[ 367, 2885, 1464, 1807],\n",
      "        [1464, 1807, 3619,  402]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=2, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e96ab0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[ 1807,  3619,   402,   271],\n",
      "        [  402,   271, 10899,  2138]]), tensor([[ 3619,   402,   271, 10899],\n",
      "        [  271, 10899,  2138,   257]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4328c33",
   "metadata": {},
   "source": [
    "Notes:<br/>\n",
    "1. Small batch sizes require less memory during training but lead to noisy model updates<br/>\n",
    "2. Batch size is a tradeoff and hyperparameter to experiment with when training LLMs.<br/>\n",
    "3. Larger strides enables to utilize the data set fully and also avoid overlap between batches. More overlap could lead to increased overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76a8c725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Target\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "# Effect of batch size\n",
    "import torch\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "input, target = next(data_iter)\n",
    "\n",
    "print(f\"Input\\n {input}\")\n",
    "print(f\"Target\\n {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe0f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
