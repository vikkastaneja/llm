{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5b5b39",
   "metadata": {},
   "source": [
    "## Read short story for tokenzation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e0f22",
   "metadata": {},
   "source": [
    "### Step 1: Create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9127b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Print total number of characters\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "\n",
    "# print first 100 characters\n",
    "print(raw_text[:99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ddf32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world!', 'This', 'is', 'a', 'test,', 'working', 'now.']\n",
      "['Hello', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test,', ' ', 'working', ' ', 'now.']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expression library\n",
    "import re\n",
    "\n",
    "text = \"Hello world! This is a test, working now.\"\n",
    "\n",
    "# Split based on white spaces: use \\s as regex\n",
    "result = re.split(r'\\s', text) # This will remove the white space\n",
    "\n",
    "print(result)\n",
    "\n",
    "result = re.split(r'(\\s)', text) # This will include the white spaces\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd75c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', ',', '', ' ', 'working', ' ', 'now', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# Now we want to include punctuation characters (commas full stops and exclamation marks) also as separate strings\n",
    "result = re.split(r'(\\W)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fae1db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'test', ',', 'working', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove empty strings and white space characters now\n",
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "517a0e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "# for the book text, there are ? -- : _ ; ' \" () are also present. Hence we need to consider them while splitting\n",
    "# Hence our tokenizer code is as follows, as of now. This is a simple tokenizer. \n",
    "# For LLMs, different tokenizer scheme is used which we will see later\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [t for t in preprocessed if t.strip()]\n",
    "print (len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec0194",
   "metadata": {},
   "source": [
    "### Step 2: Creating token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7bfe57be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "# Get all unique words and sorted\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2447b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(!, 0)\n",
      "(\", 1)\n",
      "(', 2)\n",
      "((, 3)\n",
      "(), 4)\n",
      "(,, 5)\n",
      "(--, 6)\n",
      "(., 7)\n",
      "(:, 8)\n",
      "(;, 9)\n",
      "(?, 10)\n",
      "(A, 11)\n",
      "(Ah, 12)\n",
      "(Among, 13)\n",
      "(And, 14)\n",
      "(Are, 15)\n",
      "(Arrt, 16)\n",
      "(As, 17)\n",
      "(At, 18)\n",
      "(Be, 19)\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary\n",
    "vocab = {}\n",
    "counter = 0\n",
    "for word in all_words:\n",
    "    vocab[word] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "items = list(vocab.items())[:20]\n",
    "for key,value in items:\n",
    "    print(f\"({key}, {value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4882155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a simple tokenizer class that will be used to encode the given text to feed IDs to LLM and decode the Token IDs from LLM to convert back to text.\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        # For encoder\n",
    "        self.str_to_int = vocab\n",
    "\n",
    "        # For decoder, int to string mapping\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        # Get rid of spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6551156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(vocab['\"'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a67f7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "# initialize the tokenizer\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "04aea15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7b69b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# The encoder fails when the words are not in the vocabulary\u001b[39;00m\n\u001b[32m      2\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     11\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     12\u001b[39m preprocessed = [item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()]\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "# The encoder fails when the words are not in the vocabulary\n",
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dbbe94",
   "metadata": {},
   "source": [
    "#### This shows that we need a large and diverse training set to extend the vocabulary when working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958857e",
   "metadata": {},
   "source": [
    "Another solution is to use special context tokens, user by GPT as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e036a73",
   "metadata": {},
   "source": [
    "### SPECIAL CONTEXT TOKENS <|unk|> and <|endoftext|> to existing vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36a683ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1132"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1414ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "# print last 5 entries in the updated vocab\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6076c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated existing tokenizer class to include the new special tokens\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        # For encoder\n",
    "        self.str_to_int = vocab\n",
    "\n",
    "        # For decoder, int to string mapping\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "\n",
    "        # Get rid of spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0971e294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "20114cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " '',\n",
       " ' ',\n",
       " 'do',\n",
       " ' ',\n",
       " 'you',\n",
       " ' ',\n",
       " 'like',\n",
       " ' ',\n",
       " 'tea',\n",
       " '?',\n",
       " '',\n",
       " ' ',\n",
       " '<|endoftext|>',\n",
       " ' ',\n",
       " 'In',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'sunlit',\n",
       " ' ',\n",
       " 'terraces',\n",
       " ' ',\n",
       " 'of',\n",
       " ' ',\n",
       " 'the',\n",
       " ' ',\n",
       " 'palace',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "53e0f6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1c62376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "afff1f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b0b09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
