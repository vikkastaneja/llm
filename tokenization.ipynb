{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f5b5b39",
   "metadata": {},
   "source": [
    "## Read short story for tokenzation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e0f22",
   "metadata": {},
   "source": [
    "### Step 1: Create tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9127b2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", 'r', encoding='utf-8') as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "# Print total number of characters\n",
    "print(\"Total number of characters:\", len(raw_text))\n",
    "\n",
    "# print first 100 characters\n",
    "print(raw_text[:99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ddf32a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world!', 'This', 'is', 'a', 'test,', 'working', 'now.']\n",
      "['Hello', ' ', 'world!', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test,', ' ', 'working', ' ', 'now.']\n"
     ]
    }
   ],
   "source": [
    "# Use regular expression library\n",
    "import re\n",
    "\n",
    "text = \"Hello world! This is a test, working now.\"\n",
    "\n",
    "# Split based on white spaces: use \\s as regex\n",
    "result = re.split(r'\\s', text) # This will remove the white space\n",
    "\n",
    "print(result)\n",
    "\n",
    "result = re.split(r'(\\s)', text) # This will include the white spaces\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd75c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' ', 'world', '!', '', ' ', 'This', ' ', 'is', ' ', 'a', ' ', 'test', ',', '', ' ', 'working', ' ', 'now', '.', '']\n"
     ]
    }
   ],
   "source": [
    "# Now we want to include punctuation characters (commas full stops and exclamation marks) also as separate strings\n",
    "result = re.split(r'(\\W)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae1db8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'This', 'is', 'a', 'test', ',', 'working', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "# Remove empty strings and white space characters now\n",
    "result = [item for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "517a0e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# for the book text, there are ? -- : _ ; ' \" () are also present. Hence we need to consider them while splitting\n",
    "# Hence our tokenizer code is as follows, as of now. This is a simple tokenizer. \n",
    "# For LLMs, different tokenizer scheme is used which we will see later\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [t for t in result if t.strip()]\n",
    "print (len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ec0194",
   "metadata": {},
   "source": [
    "### Step 2: Creating token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bfe57be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Get all unique words and sorted\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2447b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(!, 0)\n",
      "(,, 1)\n",
      "(., 2)\n",
      "(Hello, 3)\n",
      "(This, 4)\n",
      "(a, 5)\n",
      "(is, 6)\n",
      "(now, 7)\n",
      "(test, 8)\n",
      "(working, 9)\n",
      "(world, 10)\n"
     ]
    }
   ],
   "source": [
    "# Create a vocabulary\n",
    "vocab = {}\n",
    "counter = 0\n",
    "for word in all_words:\n",
    "    vocab[word] = counter\n",
    "    counter = counter + 1\n",
    "\n",
    "items = list(vocab.items())[:20]\n",
    "for key,value in items:\n",
    "    print(f\"({key}, {value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we build a simple tokenizer class that will be used to encode the given text to feed IDs to LLM and decode the Token IDs from LLM to convert back to text.\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        pass\n",
    "\n",
    "    def encode(self, text):\n",
    "        pass\n",
    "\n",
    "    def decode(self, ids):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
